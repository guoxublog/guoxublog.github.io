<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"guoxuliu.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="A paper list of object detection to read.">
<meta property="og:type" content="article">
<meta property="og:title" content="Object Detection Paper">
<meta property="og:url" content="https://guoxuliu.github.io/2021/09/19/object-detection/index.html">
<meta property="og:site_name" content="不忘初心">
<meta property="og:description" content="A paper list of object detection to read.">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-09-19T02:08:00.000Z">
<meta property="article:modified_time" content="2021-09-21T11:03:56.799Z">
<meta property="article:author" content="Guoxu Liu">
<meta property="article:tag" content="object detection">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://guoxuliu.github.io/2021/09/19/object-detection/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://guoxuliu.github.io/2021/09/19/object-detection/","path":"2021/09/19/object-detection/","title":"Object Detection Paper"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Object Detection Paper | 不忘初心</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">不忘初心</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">方得始终</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-首页"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-关于"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-标签"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-分类"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-归档"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Paper-List"><span class="nav-number">1.</span> <span class="nav-text">Paper List</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2020"><span class="nav-number">1.1.</span> <span class="nav-text">2020</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019"><span class="nav-number">1.2.</span> <span class="nav-text">2019</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2018"><span class="nav-number">1.3.</span> <span class="nav-text">2018</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2017"><span class="nav-number">1.4.</span> <span class="nav-text">2017</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2016"><span class="nav-number">1.5.</span> <span class="nav-text">2016</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2015"><span class="nav-number">1.6.</span> <span class="nav-text">2015</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2014"><span class="nav-number">1.7.</span> <span class="nav-text">2014</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Performance-table"><span class="nav-number">2.</span> <span class="nav-text">Performance table</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Guoxu Liu"
      src="/images/barcelona.jpg">
  <p class="site-author-name" itemprop="name">Guoxu Liu</p>
  <div class="site-description" itemprop="description">业精于勤荒于嬉</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/pandalgx" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;pandalgx" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:pandalgx@126.com" title="E-Mail → mailto:pandalgx@126.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://guoxuliu.github.io/2021/09/19/object-detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/barcelona.jpg">
      <meta itemprop="name" content="Guoxu Liu">
      <meta itemprop="description" content="业精于勤荒于嬉">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="不忘初心">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Object Detection Paper
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-09-19 10:08:00" itemprop="dateCreated datePublished" datetime="2021-09-19T10:08:00+08:00">2021-09-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-09-21 19:03:56" itemprop="dateModified" datetime="2021-09-21T19:03:56+08:00">2021-09-21</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>20k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>18 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>A paper list of object detection to read.</p>
<span id="more"></span>

<h2 id="Paper-List"><a href="#Paper-List" class="headerlink" title="Paper List"></a>Paper List</h2><h3 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h3><ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.06530.pdf">AAAI</a>] Spiking-YOLO: Spiking Neural Network for Real-time Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.08141v1.pdf">AAAI</a>] Tell Me What They’re Holding: Weakly-supervised Object Detection with Transferable Knowledge from Human-object Interaction.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.03625.pdf">AAAI</a>] Cbnet: A novel composite backbone network architecture for object detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.08287v1.pdf">AAAI</a>] Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression.</li>
<li>[<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SkxLFaNKwB">ICLR</a>] Computation Reallocation for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.10934.pdf">arXiv</a>] YOLOv4: Optimal Speed and Accuracy of Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.01998.pdf">CVPR</a>] Few-Shot Object Detection With Attention-RPN and Multi-Relation Detector.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.08455.pdf">CVPR</a>] Large-Scale Object Detection in the Wild From Imbalanced Multi-Labels.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.02424.pdf">CVPR</a>] <font color="red">Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.06493.pdf">CVPR</a>] Rethinking Classification and Localization for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.02252.pdf">CVPR</a>] Multiple Anchor Learning for Visual Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.09119.pdf">CVPR</a>] CentripetalNet: Pursuing High-Quality Keypoint Pairs for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.05086.pdf">CVPR</a>] Learning From Noisy Anchors for One-Stage Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.09070.pdf">CVPR</a>] EfficientDet: Scalable and Efficient Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Overcoming_Classifier_Imbalance_for_Long-Tail_Object_Detection_With_Balanced_Group_CVPR_2020_paper.pdf">CVPR</a>] Overcoming Classifier Imbalance for Long-Tail Object Detection With Balanced Group Softmax.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.09973.pdf">CVPR</a>] Dynamic Refinement Network for Oriented and Densely Packed Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shen_Noise-Aware_Fully_Webly_Supervised_Object_Detection_CVPR_2020_paper.pdf">CVPR</a>] Noise-Aware Fully Webly Supervised Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.11818.pdf">CVPR</a> Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cao_D2Det_Towards_High_Quality_Object_Detection_and_Instance_Segmentation_CVPR_2020_paper.pdf">CVPR</a>] D2Det: Towards High Quality Object Detection and Instance Segmentation.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.04821.pdf">CVPR</a>] Prime Sample Attention in Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.07933.pdf">CVPR</a>] Don’t Even Look Once: Synthesizing Features for Zero-Shot Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.09152.pdf">CVPR</a>] Exploring Categorical Regularization for Domain Adaptive Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Jiang_SP-NAS_Serial-to-Parallel_Backbone_Search_for_Object_Detection_CVPR_2020_paper.pdf">CVPR</a>] SP-NAS: Serial-to-Parallel Backbone Search for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.04423.pdf">CVPR</a>] NAS-FCOS: Fast Neural Architecture Search for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10156.pdf">CVPR</a>] DR Loss: Improving Object Detection by Distributional Ranking.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.09163.pdf">CVPR</a>] Detection in Crowded Scenes: One Proposal, Multiple Predictions.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.05384.pdf">CVPR</a>] AugFPN: Improving Multi-Scale Feature Learning for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Robust_Object_Detection_Under_Occlusion_With_Context-Aware_CompositionalNets_CVPR_2020_paper.pdf">CVPR</a>] Robust Object Detection Under Occlusion With Context-Aware CompositionalNets.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.13197.pdf">CVPR</a>] Cross-Domain Document Object Detection: Benchmark Suite and Method.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.09790.pdf">CVPR</a>] Exploring Bottom-Up and Top-Down Cues With Attentive Learning for Webly Supervised Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_SLV_Spatial_Likelihood_Voting_for_Weakly_Supervised_Object_Detection_CVPR_2020_paper.pdf">CVPR</a>] SLV: Spatial Likelihood Voting for Weakly Supervised Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.09231.pdf">CVPR</a>] HAMBox: Delving Into Mining High-Quality Anchors on Face Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.03538.pdf">CVPR</a>] Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.00821.pdf">CVPR</a>] Mixture Dense Regression for Object Detection and Human Pose Estimation.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Qiu_Offset_Bin_Classification_Network_for_Accurate_Object_Detection_CVPR_2020_paper.pdf">CVPR</a>] Offset Bin Classification Network for Accurate Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2001.06690.pdf">CVPR</a>] NETNet: Neighbor Erasing and Transferring Network for Better Single Shot Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.03101.pdf">CVPR</a>] Scale-Equalizing Pyramid Convolution for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://cse.buffalo.edu/~jsyuan/papers/2020/TFAN.pdf">CVPR</a>] Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.01106.pdf">CVPR</a>] MnasFPN: Learning Latency-Aware Pyramid Architecture for Object Detection on Mobile Devices.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.00543.pdf">CVPR</a>] Physically Realizable Adversarial Examples for LiDAR Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.10275.pdf">CVPR</a>] Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.04668.pdf">CVPR</a>] Incremental Few-Shot Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Luo_Where_What_Whether_Multi-Modal_Learning_Meets_Pedestrian_Detection_CVPR_2020_paper.pdf">CVPR</a>] Where, What, Whether: Multi-Modal Learning Meets Pedestrian Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.11303.pdf">CVPR</a>] Cylindrical Convolutional Networks for Joint Object Detection and Viewpoint Estimation.</li>
<li>[<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Learning_a_Unified_Sample_Weighting_Network_for_Object_Detection_CVPR_2020_paper.pdf">CVPR</a>] Learning a Unified Sample Weighting Network for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.12290.pdf">CVPR</a>] Seeing without Looking: Contextual Rescoring of Object Detections for AP Maximization.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.02334v1.pdf">arXiv</a>] DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.12872.pdf">ECCV</a>] <font color="red">End-to-End Object Detection with Transformers.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-58536-5_3">ECCV</a>] Suppress and Balance: A Simple Gated Network for Salient Object Detection. <a target="_blank" rel="noopener" href="https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency">[code]</a></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.11056.pdf">ECCV</a>] BorderDet: Border Feature for Dense Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.13816.pdf">ECCV</a>] <font color="red">Corner Proposal Network for Anchor-free, Two-stage Object Detection.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.08115v1.pdf">ECCV</a>] A General Toolbox for Understanding Errors in Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.14557.pdf">ECCV</a>] Chained-Tracker: Chaining Paired Attentive Regression Results for End-to-End Joint Multiple-Object Detection and Tracking.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.04260.pdf">ECCV</a>] Side-Aware Boundary Localization for More Precise Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.09584.pdf">ECCV</a>] PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.09336.pdf">ECCV</a>] AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian Sub-sampling.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.05643.pdf">ECCV</a>] Highly Efficient Salient Object Detection with 100K Parameters.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.10151.pdf">ECCV</a>] GeoGraph: Learning graph-based multi-view object detection with geometric cues end-to-end.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.09694.pdf">ECCV</a>] Many-shot from Low-shot: Learning to Annotate using Mixed Supervision for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.12178.pdf">ECCV</a>] Cheaper Pre-training Lunch: An Efficient Paradigm for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.05597.pdf">ECCV</a>] Arbitrary-Oriented Object Detection with Circular Smooth Label.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.12448.pdf">ECCV</a>] Soft Anchor-Point Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.06614.pdf">ECCV</a>] Object Detection with a Unified Label Space from Multiple Datasets.</li>
<li>[<a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590528.pdf">ECCV</a>] MimicDet: Bridging the Gap Between One-Stage and Two-Stage Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.00070.pdf">ECCV</a>] Prior-based Domain Adaptive Object Detection for Hazy and Rainy Conditions.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.06002.pdf">ECCV</a>] Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.06800.pdf">ECCV</a>] OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.09384.pdf">ECCV</a>] Multi-Scale Positive Sample Refinement for Few-Shot Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.12107.pdf">ECCV</a>] Few-Shot Object Detection and Viewpoint Estimation for Objects in the Wild.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.08119.pdf">ECCV</a>] Collaborative Training between Region Proposal Localization and Classification for Domain Adaptive Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://donglaiw.github.io/paper/2020_eccv_twostream.pdf">ECCV</a>] Two-Stream Active Query Suggestion for Large-Scale Object Detection in Connectomics.</li>
<li>[ECCV] FDTS: Fast Diverse-Transformation Search for Object Detection and Beyond.</li>
<li>[<a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650273.pdf">ECCV</a>] Dual refinement underwater object detection network.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.08166.pdf">ECCV</a>] APRICOT: A Dataset of Physical Adversarial Attacks on Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660477.pdf">ECCV</a>] Large Batch Optimization for Object Detection: Training COCO in 12 Minutes.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.01338.pdf">ECCV</a>] Hierarchical Context Embedding for Region-based Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.10323.pdf">ECCV</a>] Pillar-based Object Detection for Autonomous Driving.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.14350.pdf">ECCV</a>] Dive Deeper Into Box for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.01571.pdf">ECCV</a>] Domain Adaptive Object Detection via Asymmetric Tri-way Faster-RCNN.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.08103.pdf">ECCV</a>] Probabilistic Anchor Assignment with IoU Prediction for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.02355.pdf">ECCV</a>] HoughNet: Integrating near and long-range evidence for bottom-up object detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.03282.pdf">ECCV</a>] LabelEnc: A New Intermediate Supervision Method for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.07986.pdf">ECCV</a>] Boosting Weakly Supervised Object Detection with Progressive Knowledge Transfer.</li>
<li>[ECCV] On the Importance of Data Augmentation for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.12943.pdf">ECCV</a>] Adaptive Object Detection with Dual Multi-Label Prediction.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.13992.pdf">ECCV</a>] Quantum-soft QUBO Suppression for Accurate Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.09162.pdf">ECCV</a>] Improving Object Detection with Selective Self-supervised Self-training.</li>
</ul>
<h3 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h3><ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.04533.pdf">AAAI</a>] M2Det: A Single-Shot Object Detector based on Multi-Level Feature Pyramid Network. [<a target="_blank" rel="noopener" href="https://github.com/qijiezhao/M2Det">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.08225v1.pdf">AAAI</a>] Object Detection based on Region Decomposition and Assembly.</li>
<li>[<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SJgEl3A5tm">ICLR</a>] CAMOU: Learning Physical Vehicle Camouflages to Adversarially Attack Detectors in the Wild.</li>
<li>[<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SyxZJn05YX">ICLR</a>] Feature Intertwiner for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.09630.pdf">CVPR</a>] <font color="red">Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.07305.pdf">CVPR</a>] Automatic adaptation of object detectors to new domains using self-training.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02701.pdf">CVPR</a> ] Libra R-CNN: Balanced Learning for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.00621.pdf">CVPR</a>] <font color="red">Feature Selective Anchor-Free Module for Single-Shot Object Detection</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.08043.pdf">CVPR</a>] <font color="red">Bottom-up Object Detection by Grouping Extreme and Center Points.</font> [<a target="_blank" rel="noopener" href="https://github.com/xingyizhou/ExtremeNet">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.05647.pdf">CVPR</a>] C-MIL: Continuation Multiple Instance Learning for Weakly Supervised Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/AnonymousIDs/C-MIL">torch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.08425.pdf">CVPR</a>] ScratchDet: Training Single-Shot Object Detectors from Scratch.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.08545.pdf">CVPR</a>] Bounding Box Regression with Uncertainty for Accurate Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/yihui-he/KL-Loss">caffe2</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.01665.pdf">CVPR</a>] Activity Driven Weakly Supervised Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.06373.pdf">CVPR</a>] Towards Accurate One-Stage Object Detection with AP-Loss.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.04798.pdf">CVPR</a>] Strong-Weak Distribution Alignment for Adaptive Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/VisionLearningGroup/DA_Detection">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.07392.pdf">CVPR</a>] NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03629.pdf">CVPR</a>] Adaptive NMS: Refining Pedestrian Detection in a Crowd.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.01333.pdf">CVPR</a>] Point in, Box out: Beyond Counting Persons in Crowds.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.07564.pdf">CVPR</a>] <font color="red">Locating Objects Without Bounding Boxes.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.10862.pdf">CVPR</a>] Sampling Techniques for Large-Scale Object Detection from Sparsely Annotated Objects.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.04402.pdf">CVPR</a>] Towards Universal Object Detection by Domain Attention.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.05471.pdf">CVPR</a>] Exploring the Bounds of the Utility of Context for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03000.pdf">CVPR</a>] What Object Should I Use? - Task Driven Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.10016">CVPR</a>] Dissimilarity Coefficient based Weakly Supervised Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhu_Adapting_Object_Detectors_via_Selective_Cross-Domain_Alignment_CVPR_2019_paper.pdf">CVPR</a>] Adapting Object Detectors via Selective Cross-Domain Alignment.</li>
<li>[<a target="_blank" rel="noopener" href="https://yan-junjie.github.io/publication/dblp-confcvprlilqwfy-19/dblp-confcvprlilqwfy-19.pdf">CVPR</a>] Fully Quantized Network for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Distilling_Object_Detectors_With_Fine-Grained_Feature_Imitation_CVPR_2019_paper.pdf">CVPR</a>] Distilling Object Detectors with Fine-grained Feature Imitation.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Multi-Task_Self-Supervised_Object_Detection_via_Recycling_of_Bounding_Box_Annotations_CVPR_2019_paper.pdf">CVPR</a>] Multi-task Self-Supervised Object Detection via Recycling of Bounding Box Annotations.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Reasoning-RCNN_Unifying_Adaptive_Global_Reasoning_Into_Large-Scale_Object_Detection_CVPR_2019_paper.pdf">CVPR</a>] Reasoning-RCNN: Unifying Adaptive Global Reasoning into Large-scale Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.05980.pdf">CVPR</a>] Arbitrary Shape Scene Text Detection with Adaptive Text Region Representation.</li>
<li>[<a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/ec96/b6ae95e1ebbe4f7c0252301ede26dfc79467.pdf">CVPR</a>] Assisted Excitation of Activations: A Learning Technique to Improve Object Detectors.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Spatial-Aware_Graph_Relation_Network_for_Large-Scale_Object_Detection_CVPR_2019_paper.pdf">CVPR</a>] Spatial-aware Graph Relation Network for Large-scale Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Cai_MaxpoolNMS_Getting_Rid_of_NMS_Bottlenecks_in_Two-Stage_Object_Detectors_CVPR_2019_paper.pdf">CVPR</a>] MaxpoolNMS: Getting Rid of NMS Bottlenecks in Two-Stage Object Detectors.</li>
<li>[<a target="_blank" rel="noopener" href="https://web.cs.ucdavis.edu/~yjlee/projects/cvpr2019-youreapwhatyousow.pdf">CVPR</a>] You reap what you sow: Generating High Precision Object Proposals for Weakly-supervised Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Object_Detection_With_Location-Aware_Deformable_Convolution_and_Backward_Attention_Filtering_CVPR_2019_paper.pdf">CVPR</a>] Object detection with location-aware deformable convolution and backward attention filtering.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.05396.pdf">CVPR</a>] Diversify and Match: A Domain Adaptive Representation Learning Paradigm for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.07518.pdf">CVPR</a>] Hybrid Task Cascade for Instance Segmentation.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1712.00886v2.pdf">BMVC</a>] Improving Object Detection from Scratch via Gated Feature Reuse. [<a target="_blank" rel="noopener" href="https://github.com/szq0214/GFR-DSOD">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.06881v1.pdf">BMVC</a> Cascade RetinaNet: Maintaining Consistency for Single-Stage Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.06986v2.pdf">BMVC</a>] Soft Sampling for Robust Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10343v1.pdf">ICCV</a>] Multi-adversarial Faster-RCNN for Unrestricted Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10310v1.pdf">ICCV</a>] Towards Adversarially Robust Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.02361.pdf">ICCV</a>] <font color="red">A Robust Learning Approach to Domain Adaptive Object Detection.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.06368.pdf">ICCV</a>] A Delay Metric for Video Object Detection: What Average Precision Fails to Tell.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.03856.pdf">ICCV</a>] Delving Into Robust Object Detection From Unmanned Aerial Vehicles: A Deep Nuisance Disentanglement Approach.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Employing_Deep_Part-Object_Relationships_for_Salient_Object_Detection_ICCV_2019_paper.pdf">ICCV</a>] Employing Deep Part-Object Relationships for Salient Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Learning_Rich_Features_at_High-Speed_for_Single-Shot_Object_Detection_ICCV_2019_paper.pdf">ICCV</a>] Learning Rich Features at High-Speed for Single-Shot Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.04366.pdf">ICCV</a>] Structured Modeling of Joint Deep Feature and Prediction Refinement for Salient Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.10066.pdf">ICCV</a>] Selectivity or Invariance: Boundary-Aware Salient Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.09126.pdf">ICCV</a>] Progressive Sparse Local Attention for Video Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.11092.pdf">ICCV</a>] Minimum Delay Object Detection From Video.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Towards_Interpretable_Object_Detection_by_Unfolding_Latent_Structures_ICCV_2019_paper.pdf">ICCV</a>] Towards Interpretable Object Detection by Unfolding Latent Structures.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.06804.pdf">ICCV</a>] Scaling Object Detection by Transferring Classification Weights.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.01892.pdf">ICCV</a>] Scale-Aware Trident Networks for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Generative_Modeling_for_Small-Data_Object_Detection_ICCV_2019_paper.pdf">ICCV</a>] Generative Modeling for Small-Data Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://salman-h-khan.github.io/papers/ICCV19-2.pdf">ICCV</a>] Transductive Learning for Zero-Shot Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.00597.pdf">ICCV</a>] <font color="red">Self-Training and Adversarial Background Regularization for Unsupervised Domain Adaptive One-Stage Object Detection.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.08189.pdf">ICCV</a>] <font color="red">CenterNet: Keypoint Triplets for Object Detection.</font></li>
<li>[<a target="_blank" rel="noopener" href="http://www4.comp.polyu.edu.hk/~cslzhang/paper/ICCV-DAFS.pdf">ICCV</a>] Dynamic Anchor Feature Selection for Single-Shot Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Xu_Auto-FPN_Automatic_Network_Architecture_Adaptation_for_Object_Detection_Beyond_Classification_ICCV_2019_paper.pdf">ICCV</a>] Auto-FPN: Automatic Network Architecture Adaptation for Object Detection Beyond Classification.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10343.pdf">ICCV</a>] Multi-Adversarial Faster-RCNN for Unrestricted Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Deng_Object_Guided_External_Memory_Network_for_Video_Object_Detection_ICCV_2019_paper.pdf">ICCV</a>] Object Guided External Memory Network for Video Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.11752.pdf">ICCV</a>] <font color="red">ThunderNet: Towards Real-Time Generic Object Detection on Mobile Devices.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.09511.pdf">ICCV</a>] Relation Distillation Networks for Video Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.11057.pdf">ICCV</a>] Fast Object Detection in Compressed Video.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.07274.pdf">ICCV</a>] Towards High-Resolution Salient Object Detection.</li>
<li>[ICCV] Stacked Cross Refinement Network for Edge-Aware Salient Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/wuzhe71/SCAN">code</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.07061.pdf">ICCV</a>] Motion Guided Attention for Video Salient Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.04051.pdf">ICCV</a>] Semi-Supervised Video Salient Object Detection Using Pseudo-Labels.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Tan_Learning_to_Rank_Proposals_for_Object_Detection_ICCV_2019_paper.pdf">ICCV</a>] Learning to Rank Proposals for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.04972.pdf">ICCV</a>] WSOD2: Learning Bottom-Up and Top-Down Objectness Distillation for Weakly-Supervised Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.08008.pdf">ICCV</a>] Clustered Object Detection in Aerial Images.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Towards_Precise_End-to-End_Weakly_Supervised_Object_Detection_Network_ICCV_2019_paper.pdf">ICCV</a>] <font color="red">Towards Precise End-to-End Weakly Supervised Object Detection Network.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.01866.pdf">ICCV</a>] Few-Shot Object Detection via Feature Reweighting.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Shao_Objects365_A_Large-Scale_High-Quality_Dataset_for_Object_Detection_ICCV_2019_paper.pdf">ICCV</a>] Objects365: A Large-Scale, High-Quality Dataset for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.08297.pdf">ICCV</a>] EGNet: Edge Guidance Network for Salient Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.07567.pdf">ICCV</a>] Optimizing the F-Measure for Threshold-Free Salient Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.06390.pdf">ICCV</a>] Sequence Level Semantics Aggregation for Video Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.00124.pdf">ICCV</a>] <font color="red">NOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-Supervised Object Detection.</font></li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Nie_Enriched_Feature_Guided_Refinement_Network_for_Object_Detection_ICCV_2019_paper.pdf">ICCV</a>] Enriched Feature Guided Refinement Network for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.02225.pdf">ICCV</a>] POD: Practical Object Detection With Scale-Sensitive Network.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.01355.pdf">ICCV</a>] <font color="red">FCOS: Fully Convolutional One-Stage Object Detection.</font></li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.11490.pdf">ICCV</a>] RepPoints: Point Set Representation for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Noh_Better_to_Follow_Follow_to_Be_Better_Towards_Precise_Supervision_ICCV_2019_paper.pdf">ICCV</a>] Better to Follow, Follow to Be Better: Towards Precise Supervision of Feature Super-Resolution for Small Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.00551.pdf">ICCV</a>] Weakly Supervised Object Detection With Segmentation Collaboration.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Shvets_Leveraging_Long-Range_Temporal_Relationships_Between_Proposals_for_Video_Object_Detection_ICCV_2019_paper.pdf">ICCV</a>] Leveraging Long-Range Temporal Relationships Between Proposals for Video Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.05217.pdf">ICCV</a>] Detecting 11K Classes: Large Scale Object Detection Without Fine-Grained Bounding Boxes.</li>
<li>[ICCV] C-MIDN: Coupled Multiple Instance Detection Network With Segmentation Guidance for Weakly Supervised Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Meta-Learning_to_Detect_Rare_Objects_ICCV_2019_paper.pdf">ICCV</a>] Meta-Learning to Detect Rare Objects.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.10164v1.pdf">ICCV</a>] Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.04620.pdf">ICCV</a>] Gaussian YOLOv3: An Accurate and Fast Object Detector using Localization Uncertainty for Autonomous Driving. [<a target="_blank" rel="noopener" href="https://github.com/jwchoi384/Gaussian_YOLOv3">c</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.02466v1.pdf">NIPS</a>] <font color="red">FreeAnchor: Learning to Match Anchors for Visual Object Detection.</font></li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8376-memory-oriented-decoder-for-light-field-salient-object-detection.pdf">NIPS</a>] Memory-oriented Decoder for Light Field Salient Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8540-one-shot-object-detection-with-co-attention-and-co-excitation.pdf">NIPS</a>] One-Shot Object Detection with Co-Attention and Co-Excitation.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1903.10979v4.pdf">NIPS</a>] DetNAS: Backbone Search for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/9259-consistency-based-semi-supervised-learning-for-object-detection.pdf">NIPS</a>] Consistency-based Semi-supervised Learning for Object detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.02293.pdf">NIPS</a>] Efficient Neural Architecture Transformation Searchin Channel-Level for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.11172.pdf">arXiv</a>] Learning Data Augmentation Strategies for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.05027.pdf">arXiv</a>] Spinenet: Learning scale-permuted backbone for recognition and localization.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.05055.pdf">arXiv</a>] <font color="red">Object Detection in 20 Years: A Survey.</font></li>
</ul>
<h3 id="2018"><a href="#2018" class="headerlink" title="2018"></a>2018</h3><ul>
<li>[<a target="_blank" rel="noopener" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">arXiv</a>] <font color="red">YOLOv3: An Incremental Improvement.</font> [<a target="_blank" rel="noopener" href="https://pjreddie.com/darknet/yolo/">c</a>] [<a target="_blank" rel="noopener" href="https://github.com/ayooshkathuria/pytorch-yolo-v3">pytorch</a>] [<a target="_blank" rel="noopener" href="https://github.com/eriklindernoren/PyTorch-YOLOv3">pytorch</a>] [<a target="_blank" rel="noopener" href="https://github.com/qqwweee/keras-yolo3">keras</a>] [<a target="_blank" rel="noopener" href="https://github.com/mystic123/tensorflow-yolo-v3">tensorflow</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1702.05711.pdf">IJCV</a>] Zoom Out-and-In Network with Recursive Training for Object Proposal. [<a target="_blank" rel="noopener" href="https://github.com/hli2020/zoom_network">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Structure_Inference_Net_CVPR_2018_paper.pdf">CVPR</a>] Structure Inference Net: Object Detection Using Scene-Level Context and Instance-Level Relationships. [<a target="_blank" rel="noopener" href="https://github.com/choasup/SIN">tensorflow</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Scale-Transferrable_Object_Detection_CVPR_2018_paper.pdf">CVPR</a>] Scale-Transferrable Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Single-Shot_Refinement_Neural_CVPR_2018_paper.pdf">CVPR</a>] Single-Shot Refinement Neural Network for Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/sfzhang15/RefineDet">caffe</a>] [<a target="_blank" rel="noopener" href="https://github.com/fukatani/RefineDet_chainer">chainer</a>] [<a target="_blank" rel="noopener" href="https://github.com/lzx1413/PytorchSSD">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Peng_MegDet_A_Large_CVPR_2018_paper.pdf">CVPR</a>] MegDet: A Large Mini-Batch Object Detector.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Domain_Adaptive_Faster_CVPR_2018_paper.pdf">CVPR</a>] Domain Adaptive Faster R-CNN for Object Detection in the Wild. [<a target="_blank" rel="noopener" href="https://github.com/yuhuayc/da-faster-rcnn">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.08189.pdf">CVPR</a>] An Analysis of Scale Invariance in Object Detection – SNIP.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.11575.pdf">CVPR</a>] Relation Networks for Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/msracver/Relation-Networks-for-Object-Detection">mxnet</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Cai_Cascade_R-CNN_Delving_CVPR_2018_paper.pdf">CVPR</a>] Cascade R-CNN: Delving into High Quality Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/zhaoweicai/cascade-rcnn">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://ivul.kaust.edu.sa/Documents/Publications/2018/Finding%20Tiny%20Faces%20in%20the%20Wild%20with%20Generative%20Adversarial%20Network.pdf">CVPR</a>] Finding Tiny Faces in the Wild with Generative Adversarial Network.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.00428.pdf">CVPR</a>] Multi-scale Location-aware Kernel Representation for Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/Hwang64/MLKP">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.11365.pdf">CVPR</a>] Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation. [<a target="_blank" rel="noopener" href="https://github.com/naoto0804/cross-domain-detection">chainer</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0794.pdf">CVPR</a>] Improving Object Localization with Fitness NMS and Bounded IoU Loss.</li>
<li>[<a target="_blank" rel="noopener" href="http://bmvc2018.org/contents/papers/0897.pdf">BMVC</a>] STDnet: A ConvNet for Small Target Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.07767.pdf">ECCV</a>] Receptive Field Block Net for Accurate and Fast Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/ruinmessi/RFBNet">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Qingyi_Tao_Zero-Annotation_Object_Detection_ECCV_2018_paper.pdf">ECCV</a>] Zero-Annotation Object Detection with Web Knowledge Transfer.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.01244.pdf">ECCV</a>] <font color="red">CornerNet: Detecting Objects as Paired Keypoints.</font> [<a target="_blank" rel="noopener" href="https://github.com/princeton-vl/CornerNet">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Seung-Wook_Kim_Parallel_Feature_Pyramid_ECCV_2018_paper.pdf">ECCV</a>] Parallel Feature Pyramid Network for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.08545.pdf">arXiv</a>] Softer-NMS: Rethinking Bounding Box Regression for Accurate Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.05810.pdf">ECML-PKDD</a>] ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector. [<a target="_blank" rel="noopener" href="https://github.com/shangtse/robust-physical-attack">tensorflow</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/7466-pelee-a-real-time-object-detection-system-on-mobile-devices.pdf">NIPS</a>] Pelee: A Real-Time Object Detection System on Mobile Devices. [<a target="_blank" rel="noopener" href="https://github.com/Robert-JunWang/Pelee">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/7428-hybrid-knowledge-routed-modules-for-large-scale-object-detection.pdf">NIPS</a>] Hybrid Knowledge Routed Modules for Large-scale Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/7315-metaanchor-learning-to-detect-objects-with-customized-anchors.pdf">NIPS</a> MetaAnchor: Learning to Detect Objects with Customized Anchors.</li>
<li>[<a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/8143-sniper-efficient-multi-scale-training.pdf">NIPS</a>] SNIPER: Efficient Multi-Scale Training.</li>
</ul>
<h3 id="2017"><a href="#2017" class="headerlink" title="2017"></a>2017</h3><ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1701.06659.pdf">arXiv</a>] DSSD : Deconvolutional Single Shot Detector. [<a target="_blank" rel="noopener" href="https://github.com/chengyangfu/caffe/tree/dssd">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.06851.pdf">CVPR</a>] Beyond Skip Connections: Top-Down Modulation for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf">CVPR</a>] <font color="red">Feature Pyramid Networks for Object Detection.</font> [<a target="_blank" rel="noopener" href="https://github.com/unsky/FPN">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1612.08242.pdf">CVPR</a>] <font color="red">YOLO9000: Better, Faster, Stronger.</font> [<a target="_blank" rel="noopener" href="https://pjreddie.com/darknet/yolo/">c</a>] [<a target="_blank" rel="noopener" href="https://github.com/quhezheng/caffe_yolo_v2">caffe</a>] [<a target="_blank" rel="noopener" href="https://github.com/nilboy/tensorflow-yolo">tensorflow</a>] [<a target="_blank" rel="noopener" href="https://github.com/sualab/object-detection-yolov2">tensorflow</a>] [<a target="_blank" rel="noopener" href="https://github.com/longcw/yolo2-pytorch">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1707.01691.pdf">CVPR</a>] RON: Reverse Connection with Objectness Prior Networks for Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/taokong/RON">caffe</a>] [<a target="_blank" rel="noopener" href="https://github.com/HiKapok/RON_Tensorflow">tensorflow</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1707.09531.pdf">ICCV</a>] Recurrent Scale Approximation for Object Detection in CNN. [<a target="_blank" rel="noopener" href="https://github.com/sciencefans/RSA-for-object-detection">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Dai_Deformable_Convolutional_Networks_ICCV_2017_paper.pdf">ICCV</a>] Deformable Convolutional Networks. [<a target="_blank" rel="noopener" href="https://github.com/msracver/Deformable-ConvNets">mxnet</a>] [<a target="_blank" rel="noopener" href="https://github.com/Zardinality/TF_Deformable_Net">tensorflow</a>] [<a target="_blank" rel="noopener" href="https://github.com/oeway/pytorch-deform-conv">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.10295.pdf">ICCV</a>] DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling. [<a target="_blank" rel="noopener" href="https://github.com/lachlants/denet">theano</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.02863.pdf">ICCV</a>] CoupleNet: Coupling Global Structure with Local Parts for Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/tshizys/CoupleNet">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.02002.pdf">ICCV</a>] <font color="red">Focal Loss for Dense Object Detection.</font> [<a target="_blank" rel="noopener" href="https://github.com/fizyr/keras-retinanet">keras</a>] [<a target="_blank" rel="noopener" href="https://github.com/kuangliu/pytorch-retinanet">pytorch</a>] [<a target="_blank" rel="noopener" href="https://github.com/unsky/RetinaNet">mxnet</a>] [<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tpu/tree/master/models/official/retinanet">tensorflow</a>]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf">ICCV</a>] <font color="red">Mask R-CNN.</font> [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/Detectron">caffe2</a>] [<a target="_blank" rel="noopener" href="https://github.com/matterport/Mask_RCNN">tensorflow</a>] [<a target="_blank" rel="noopener" href="https://github.com/CharlesShang/FastMaskRCNN">tensorflow</a>] [<a target="_blank" rel="noopener" href="https://github.com/multimodallearning/pytorch-mask-rcnn">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.01241.pdf">ICCV</a>] DSOD: Learning Deeply Supervised Object Detectors from Scratch. [<a target="_blank" rel="noopener" href="https://github.com/szq0214/DSOD">caffe</a>] [<a target="_blank" rel="noopener" href="https://github.com/uoip/SSD-variants">pytorch</a> ]</li>
<li>[<a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Chen_Spatial_Memory_for_ICCV_2017_paper.pdf">ICCV</a>] Spatial Memory for Context Reasoning in Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.07264.pdf">arXiv</a>] Light-Head R-CNN: In Defense of Two-Stage Object Detector. [<a target="_blank" rel="noopener" href="https://github.com/zengarden/light_head_rcnn">tensorflow</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1704.04503.pdf">ICCV</a>] Improving Object Detection With One Line of Code. [<a target="_blank" rel="noopener" href="https://github.com/bharatsingh430/soft-nms">caffe</a>]</li>
</ul>
<h3 id="2016"><a href="#2016" class="headerlink" title="2016"></a>2016</h3><ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.02640.pdf">CVPR</a>] <font color="red">You Only Look Once: Unified, Real-Time Object Detection.</font> [<a target="_blank" rel="noopener" href="https://pjreddie.com/darknet/yolo/">c</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.07729.pdf">CVPR</a>] G-CNN: an Iterative Grid Based Object Detector.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.07711.pdf">CVPR</a>] Adaptive Object Detection Using Adjacency and Zoom Prediction.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.04143.pdf">CVPR</a>] Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1604.00600.pdf">CVPR</a>] HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1604.03540.pdf">CVPR</a>] Training Region-based Object Detectors with Online Hard Example Mining. [<a target="_blank" rel="noopener" href="https://github.com/abhi2610/ohem">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1604.03239.pdf">CVPR</a>] CRAFT Objects from Images. [<a target="_blank" rel="noopener" href="https://github.com/byangderek/CRAFT">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1604.02135.pdf">BMVC</a>] A MultiPath Network for Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/multipathnet">torch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.02325.pdf">ECCV</a>] SSD: Single Shot MultiBox Detector. [<a target="_blank" rel="noopener" href="https://github.com/weiliu89/caffe/tree/ssd">caffe</a>] [<a target="_blank" rel="noopener" href="https://github.com/balancap/SSD-Tensorflow">tensorflow</a>] [<a target="_blank" rel="noopener" href="https://github.com/amdegroot/ssd.pytorch">pytorch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1610.02579.pdf">ECCV</a>] Crafting GBD-Net for Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/craftGBD/craftGBD">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/40e7/4473cb82231559cbaeaa44989e9bbfe7ec3f.pdf">ECCV</a>] Contextual Priming and Feedback for Faster R-CNN.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1607.07155.pdf">ECCV</a>] A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/zhaoweicai/mscnn">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1605.06409.pdf">NIPS</a>] R-FCN: Object Detection via Region-based Fully Convolutional Networks. [<a target="_blank" rel="noopener" href="https://github.com/daijifeng001/R-FCN">caffe</a>] [<a target="_blank" rel="noopener" href="https://github.com/YuwenXiong/py-R-FCN">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1608.08021.pdf">NIPSW</a>] PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection. [<a target="_blank" rel="noopener" href="https://github.com/sanghoon/pva-faster-rcnn">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1412.5661.pdf">TPAMI</a>] DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1504.06066.pdf">TPAMI</a>] Object Detection Networks on Convolutional Feature Maps.</li>
</ul>
<h3 id="2015"><a href="#2015" class="headerlink" title="2015"></a>2015</h3><ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1504.03293.pdf">CVPR</a>] Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction. [<a target="_blank" rel="noopener" href="https://github.com/YutingZhang/fgs-obj">matlab</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Gidaris_Object_Detection_via_ICCV_2015_paper.pdf">ICCV</a>] Object detection via a multi-region &amp; semantic segmentation-aware CNN model. [<a target="_blank" rel="noopener" href="https://github.com/gidariss/mrcnn-object-detection">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1505.02146.pdf">ICCV</a>] DeepBox: Learning Objectness with Convolutional Networks. [<a target="_blank" rel="noopener" href="https://github.com/weichengkuo/DeepBox">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.07704.pdf">ICCV</a>] AttentionNet: Aggregating Weak Directions for Accurate Object Detection.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1504.08083.pdf">ICCV</a>] Fast R-CNN. [<a target="_blank" rel="noopener" href="https://github.com/rbgirshick/fast-rcnn">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1510.04445.pdf">ICCV</a>] DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers. [<a target="_blank" rel="noopener" href="https://github.com/aghodrati/deepproposal">matconvnet</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">NIPS</a>] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. [<a target="_blank" rel="noopener" href="https://github.com/rbgirshick/py-faster-rcnn">caffe</a>] [<a target="_blank" rel="noopener" href="https://github.com/endernewton/tf-faster-rcnn">tensorflow</a>] [<a target="_blank" rel="noopener" href="https://github.com/jwyang/faster-rcnn.pytorch">pytorch</a>]</li>
</ul>
<h3 id="2014"><a href="#2014" class="headerlink" title="2014"></a>2014</h3><ul>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1311.2524.pdf">CVPR</a>] Rich feature hierarchies for accurate object detection and semantic segmentation. [<a target="_blank" rel="noopener" href="https://github.com/rbgirshick/rcnn">caffe</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1312.6229.pdf">ICLR</a>] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. [<a target="_blank" rel="noopener" href="https://github.com/sermanet/OverFeat">torch</a>]</li>
<li>[<a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf">CVPR</a>] Scalable Object Detection using Deep Neural Networks.</li>
<li>[<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1406.4729.pdf">ECCV</a>] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. [<a target="_blank" rel="noopener" href="https://github.com/ShaoqingRen/SPP_net">caffe</a>] [<a target="_blank" rel="noopener" href="https://github.com/yhenon/keras-spp">keras</a>] [<a target="_blank" rel="noopener" href="https://github.com/peace195/sppnet">tensorflow</a>]</li>
</ul>
<h2 id="Performance-table"><a href="#Performance-table" class="headerlink" title="Performance table"></a>Performance table</h2><p>FPS (Speed) index is related to the hardware spec(e.g. CPU, GPU, RAM, etc), so it is hard to make an equal comparison. The solution is to measure the performance of all models on hardware with equivalent specifications, but it is very difficult and time consuming. </p>
<table>
<thead>
<tr>
<th align="center">Detector</th>
<th align="center">VOC07 (mAP@IoU=0.5)</th>
<th align="center">VOC12 (mAP@IoU=0.5)</th>
<th align="center">COCO (mAP@IoU=0.5:0.95)</th>
<th align="center">Published In</th>
</tr>
</thead>
<tbody><tr>
<td align="center">R-CNN</td>
<td align="center">58.5</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">CVPR’14</td>
</tr>
<tr>
<td align="center">SPP-Net</td>
<td align="center">59.2</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">ECCV’14</td>
</tr>
<tr>
<td align="center">MR-CNN</td>
<td align="center">78.2 (07+12)</td>
<td align="center">73.9 (07+12)</td>
<td align="center">-</td>
<td align="center">ICCV’15</td>
</tr>
<tr>
<td align="center">Fast R-CNN</td>
<td align="center">70.0 (07+12)</td>
<td align="center">68.4 (07++12)</td>
<td align="center">19.7</td>
<td align="center">ICCV’15</td>
</tr>
<tr>
<td align="center">Faster R-CNN</td>
<td align="center">73.2 (07+12)</td>
<td align="center">70.4 (07++12)</td>
<td align="center">21.9</td>
<td align="center">NIPS’15</td>
</tr>
<tr>
<td align="center">YOLO v1</td>
<td align="center">66.4 (07+12)</td>
<td align="center">57.9 (07++12)</td>
<td align="center">-</td>
<td align="center">CVPR’16</td>
</tr>
<tr>
<td align="center">G-CNN</td>
<td align="center">66.8</td>
<td align="center">66.4 (07+12)</td>
<td align="center">-</td>
<td align="center">CVPR’16</td>
</tr>
<tr>
<td align="center">AZNet</td>
<td align="center">70.4</td>
<td align="center">-</td>
<td align="center">22.3</td>
<td align="center">CVPR’16</td>
</tr>
<tr>
<td align="center">ION</td>
<td align="center">80.1</td>
<td align="center">77.9</td>
<td align="center">33.1</td>
<td align="center">CVPR’16</td>
</tr>
<tr>
<td align="center">HyperNet</td>
<td align="center">76.3 (07+12)</td>
<td align="center">71.4 (07++12)</td>
<td align="center">-</td>
<td align="center">CVPR’16</td>
</tr>
<tr>
<td align="center">OHEM</td>
<td align="center">78.9 (07+12)</td>
<td align="center">76.3 (07++12)</td>
<td align="center">22.4</td>
<td align="center">CVPR’16</td>
</tr>
<tr>
<td align="center">MPN</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">33.2</td>
<td align="center">BMVC’16</td>
</tr>
<tr>
<td align="center">SSD</td>
<td align="center">76.8 (07+12)</td>
<td align="center">74.9 (07++12)</td>
<td align="center">31.2</td>
<td align="center">ECCV’16</td>
</tr>
<tr>
<td align="center">GBDNet</td>
<td align="center">77.2 (07+12)</td>
<td align="center">-</td>
<td align="center">27.0</td>
<td align="center">ECCV’16</td>
</tr>
<tr>
<td align="center">CPF</td>
<td align="center">76.4 (07+12)</td>
<td align="center">72.6 (07++12)</td>
<td align="center">-</td>
<td align="center">ECCV’16</td>
</tr>
<tr>
<td align="center">R-FCN</td>
<td align="center">79.5 (07+12)</td>
<td align="center">77.6 (07++12)</td>
<td align="center">29.9</td>
<td align="center">NIPS’16</td>
</tr>
<tr>
<td align="center">DeepID-Net</td>
<td align="center">69.0</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">PAMI’16</td>
</tr>
<tr>
<td align="center">NoC</td>
<td align="center">71.6 (07+12)</td>
<td align="center">68.8 (07+12)</td>
<td align="center">27.2</td>
<td align="center">TPAMI’16</td>
</tr>
<tr>
<td align="center">DSSD</td>
<td align="center">81.5 (07+12)</td>
<td align="center">80.0 (07++12)</td>
<td align="center">33.2</td>
<td align="center">arXiv’17</td>
</tr>
<tr>
<td align="center">TDM</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">37.3</td>
<td align="center">CVPR’17</td>
</tr>
<tr>
<td align="center">FPN</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">36.2</td>
<td align="center">CVPR’17</td>
</tr>
<tr>
<td align="center">YOLO v2</td>
<td align="center">78.6 (07+12)</td>
<td align="center">73.4 (07++12)</td>
<td align="center">-</td>
<td align="center">CVPR’17</td>
</tr>
<tr>
<td align="center">RON</td>
<td align="center">77.6 (07+12)</td>
<td align="center">75.4 (07++12)</td>
<td align="center">27.4</td>
<td align="center">CVPR’17</td>
</tr>
<tr>
<td align="center">DeNet</td>
<td align="center">77.1 (07+12)</td>
<td align="center">73.9 (07++12)</td>
<td align="center">33.8</td>
<td align="center">ICCV’17</td>
</tr>
<tr>
<td align="center">CoupleNet</td>
<td align="center">82.7 (07+12)</td>
<td align="center">80.4 (07++12)</td>
<td align="center">34.4</td>
<td align="center">ICCV’17</td>
</tr>
<tr>
<td align="center">RetinaNet</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">39.1</td>
<td align="center">ICCV’17</td>
</tr>
<tr>
<td align="center">DSOD</td>
<td align="center">77.7 (07+12)</td>
<td align="center">76.3 (07++12)</td>
<td align="center">-</td>
<td align="center">ICCV’17</td>
</tr>
<tr>
<td align="center">SMN</td>
<td align="center">70.0</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">ICCV’17</td>
</tr>
<tr>
<td align="center">Light-Head R-CNN</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">41.5</td>
<td align="center">arXiv’17</td>
</tr>
<tr>
<td align="center">YOLO v3</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">33.0</td>
<td align="center">arXiv’18</td>
</tr>
<tr>
<td align="center">SIN</td>
<td align="center">76.0 (07+12)</td>
<td align="center">73.1 (07++12)</td>
<td align="center">23.2</td>
<td align="center">CVPR’18</td>
</tr>
<tr>
<td align="center">STDN</td>
<td align="center">80.9 (07+12)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">CVPR’18</td>
</tr>
<tr>
<td align="center">RefineDet</td>
<td align="center">83.8 (07+12)</td>
<td align="center">83.5 (07++12)</td>
<td align="center">41.8</td>
<td align="center">CVPR’18</td>
</tr>
<tr>
<td align="center">SNIP</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">45.7</td>
<td align="center">CVPR’18</td>
</tr>
<tr>
<td align="center">Relation-Network</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">32.5</td>
<td align="center">CVPR’18</td>
</tr>
<tr>
<td align="center">Cascade R-CNN</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">42.8</td>
<td align="center">CVPR’18</td>
</tr>
<tr>
<td align="center">MLKP</td>
<td align="center">80.6 (07+12)</td>
<td align="center">77.2 (07++12)</td>
<td align="center">28.6</td>
<td align="center">CVPR’18</td>
</tr>
<tr>
<td align="center">Fitness-NMS</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">41.8</td>
<td align="center">CVPR’18</td>
</tr>
<tr>
<td align="center">RFBNet</td>
<td align="center">82.2 (07+12)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">ECCV’18</td>
</tr>
<tr>
<td align="center">CornerNet</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">42.1</td>
<td align="center">ECCV’18</td>
</tr>
<tr>
<td align="center">PFPNet</td>
<td align="center">84.1 (07+12)</td>
<td align="center">83.7 (07++12)</td>
<td align="center">39.4</td>
<td align="center">ECCV’18</td>
</tr>
<tr>
<td align="center">Pelee</td>
<td align="center">70.9 (07+12)</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">NIPS’18</td>
</tr>
<tr>
<td align="center">HKRM</td>
<td align="center">78.8 (07+12)</td>
<td align="center">-</td>
<td align="center">37.8</td>
<td align="center">NIPS’18</td>
</tr>
<tr>
<td align="center">M2Det</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">44.2</td>
<td align="center">AAAI’19</td>
</tr>
<tr>
<td align="center">R-DAD</td>
<td align="center">81.2 (07++12)</td>
<td align="center">82.0 (07++12)</td>
<td align="center">43.1</td>
<td align="center">AAAI’19</td>
</tr>
<tr>
<td align="center">ScratchDet</td>
<td align="center">84.1 (07++12)</td>
<td align="center">83.6 (07++12)</td>
<td align="center">39.1</td>
<td align="center">CVPR’19</td>
</tr>
<tr>
<td align="center">Libra R-CNN</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">43.0</td>
<td align="center">CVPR’19</td>
</tr>
<tr>
<td align="center">Reasoning-RCNN</td>
<td align="center">82.5 (07++12)</td>
<td align="center">-</td>
<td align="center">43.2</td>
<td align="center">CVPR’19</td>
</tr>
<tr>
<td align="center">FSAF</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">44.6</td>
<td align="center">CVPR’19</td>
</tr>
<tr>
<td align="center">AmoebaNet + NAS-FPN</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">47.0</td>
<td align="center">CVPR’19</td>
</tr>
<tr>
<td align="center">Cascade-RetinaNet</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">41.1</td>
<td align="center">CVPR’19</td>
</tr>
<tr>
<td align="center">HTC</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">47.2</td>
<td align="center">CVPR’19</td>
</tr>
<tr>
<td align="center">TridentNet</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">48.4</td>
<td align="center">ICCV’19</td>
</tr>
<tr>
<td align="center">DAFS</td>
<td align="center"><strong>85.3 (07+12)</strong></td>
<td align="center">83.1 (07++12)</td>
<td align="center">40.5</td>
<td align="center">ICCV’19</td>
</tr>
<tr>
<td align="center">Auto-FPN</td>
<td align="center">81.8 (07++12)</td>
<td align="center">-</td>
<td align="center">40.5</td>
<td align="center">ICCV’19</td>
</tr>
<tr>
<td align="center">FCOS</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">44.7</td>
<td align="center">ICCV’19</td>
</tr>
<tr>
<td align="center">FreeAnchor</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">44.8</td>
<td align="center">NeurIPS’19</td>
</tr>
<tr>
<td align="center">DetNAS</td>
<td align="center">81.5 (07++12)</td>
<td align="center">-</td>
<td align="center">42.0</td>
<td align="center">NeurIPS’19</td>
</tr>
<tr>
<td align="center">NATS</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">42.0</td>
<td align="center">NeurIPS’19</td>
</tr>
<tr>
<td align="center">AmoebaNet + NAS-FPN + AA</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">50.7</td>
<td align="center">arXiv’19</td>
</tr>
<tr>
<td align="center">SpineNet</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">52.1</td>
<td align="center">arXiv’19</td>
</tr>
<tr>
<td align="center">CBNet</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">53.3</td>
<td align="center">AAAI’20</td>
</tr>
<tr>
<td align="center">EfficientDet</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">52.6</td>
<td align="center">CVPR’20</td>
</tr>
<tr>
<td align="center">DetectoRS</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center"><strong>54.7</strong></td>
<td align="center">arXiv’20</td>
</tr>
</tbody></table>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Guoxu Liu
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://guoxuliu.github.io/2021/09/19/object-detection/" title="Object Detection Paper">https://guoxuliu.github.io/2021/09/19/object-detection/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/object-detection/" rel="tag"># object detection</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/09/09/build-hexo-blog/" rel="prev" title="使用Hexo在GitHub上搭建博客">
                  <i class="fa fa-chevron-left"></i> 使用Hexo在GitHub上搭建博客
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guoxu Liu</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">22k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">20 分钟</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"guoxublog","repo":"blog_comment","client_id":"3bad4a190b4ca8d2ff04","client_secret":"9af9eff158b4ed90eed71e82cac2d43c2ea90f1d","admin_user":"guoxublog","distraction_free_mode":false,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"21b18428ea53b1eff3356933c033fa43"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
